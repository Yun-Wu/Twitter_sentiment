\section{Problem Definition and Algorithm}
\label{sec:def}

\subsection{Task Definition}
In this paper, we address the problem of sentiment analysis of Twitter message. To be more precise, 
given a message, we want to classify whether the message is of positive or negative (binary decision), or neutral sentiment (ternary decision). For messages conveying both a positive and negative sentiment, whichever is the stronger sentiment should be chosen. For the following two example messages, we are expected to return positive on the first one and negative on the second. 
\begin{mdframed}[
  leftmargin=\parindent,
  rightmargin=\parindent,
  skipabove=\topsep,
  skipbelow=\topsep
  ]
  If u haven't seen \#Rio2 yet-GO! You need to meet Gabi! Great singer. Cute. Absolutely hysterical! @KChenoweth \url{pic.twitter.com/kkVBUKjqE3}
\end{mdframed}

\begin{mdframed}[
  leftmargin=\parindent,
  rightmargin=\parindent,
  skipabove=\topsep,
  skipbelow=\topsep
  ]
 The rio 2 has one of the worst soundtracks evvvvaaa. I'm at Alamo @Drafthouse Cinema. @marissanicole11 \url{http://4sq.com/1kKF8qE} 
\end{mdframed}

This is an interesting task because sentiment of Twitter message can be used as a barometer for public mood and opinion in diverse areas such as entertainment, politics and economics. For example, 
it was used to provide information on the temporal dynamic of sentiment in reaction to the debate video between Barack Obama and John McCain\cite{Diakopoulos:2010}. 
There is also a report on "Berkshire Hathaway Stock Rises When Anne Hathaway Makes Headlines"\footnote{\url{http://goo.gl/WlfY1c}/}, which indicates that sentiment toward public figure may have potential influence over stock market. 

However, twitter message presents greater challenges for sentiment analysis than traditional text genres, such as newswire data.  Tweets are within 140 characters, often consists of a few short sentences or even a single sentence. 
The language used is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, such as, RT for “re-tweet” and \#hashtags, which are a type of tagging for Twitter messages. \footnote{\url{http://www.cs.york.ac.uk/semeval-2013/task2/}}


\subsection{Algorithm Definition}
We experiment with two different types of models: The first is the traditional bag-of-word model, namely SVM here. The other is the recently proposed Recursive Neural Tensor Network which works pretty good on movie reviews. As different model has different requirement on pre-processing, we first introduce the models we use and then the pre-processing steps. 

\subsubsection{Pre-processing}
The Twitter language is known as informal and flexible. Such properties will provide information about sentiment and should be processed carefully. As the SVM and RNTN model are build upon two different basic ideas, we need to have different pre-processing step for each model. For SVM, our goal is to maximize the information about sentiment while reduce the sparsity of the feature vector. For RNTN, we try to formalize tweets to well-organized sentences such that Stanford parser can recognize it well.

\begin{description}
\item[General features] First we replace all the remaining HTML entities like ``\&amp;'' to ASCII code ``-''. We also delete the quotes around the whole tweet. Then we transform all the URL address to the keyword ``URL'' and the user names(start with @) to the key word ``target'' to keep the structure of the sentences. Hashtags may contain sentimental information so we simply remove the ``\#'' in the front of the hashtag.
\item[Preprocessing with dictionary] Tweets contain very casual language. One common phenomenon is lengthening word. For example, instead of ``so'', Twitter users may repeat 'o's to express their intense feeling as ``sooo'' or ``sooooooo''. We transform all the lengthening words to three repeated letters like ``sooo''. There are also many misspellings and slangs in Twitter and we use a slang dictionary to formalize it.
\item[Preprocessing for SVM] We extract additional features for SVM and attach ``\_neg'' to negated sentences. Detailed information can be found in Section \ref{svm}.  %2.2.1.

\item[Word Cluster] We also try to reduce feature space with word cluster \cite{Owoputi:2013}. Firstly, we obtain hierarchical word clusters via Brown clustering. The algorithm partitions words into a base set of 1,000 clusters, and induces a hierarchy among those 1,000 clusters with a series of greedy agglomerative merges that heuristically optimize the likelihood of a hidden Markov model with a one-class-per-lexical-type constraint. Then we sort the word in each cluster with frequency and use the most frequent one to represent the sentiment of the cluster.


\item[Preprocessing for RNTN] Since RNTN is based on Stanford parser, we need to preserve the structure of sentences while filtering out noisy. 
 Emoticons, for example, can not be easily organized by our parser but provide sentimental information. We split each emoticon with hat, eyes, nose and mouth and replace it with the corresponding adverb. For instance, ``:-)'' will be replaced with ``happily'', ``:O'' with be substituted as ``surprisedly''.  To reduce errors in sentence splitter, multiple punctuation like ``!!!!'' will be shortened to ``!''.  A period is added to each tweet that does not end with punctuation.

%\begin{verbatim}
%^001010110 never neva nvr nevr #never neverr nver neverrr nevaa neeever nevet neeeever nevva
%\end{verbatim}
%
%But it also integrates antonyms, which is fatal for our task.
%\begin{verbatim}
%^111100010011 sad blessed frightening sad lucky frustrating helpful impressed low steady :((( d&d busy
%\end{verbatim}
\end{description}


\subsubsection{SVM}
\label{svm}
A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. We use the SVM$^{\text{libSVM}}$ software with a linear and Radial Basis Function (RBF) kernel. We input two sets of vectors of size $m$. Each entry in the vector corresponds to the presence of a feature. For example, with a unigram feature extractor, each feature is a single word found in a tweet. If this single word also appears in our dictionary, the value is $1$, otherwise the value is $0$. In our experiment, we evaluate several different methods to stand for the presence of one feature. 



\newpage

\subsubsection{RNTN}
In Recursive Neural Tensor Network, each word is represented as a $d-$dimensional vector. When an $n-$gram is given to the compositional models, it is parsed into a binary tree (as in Figure \ref{trigram}). We compute the parent vector in a bottom up fashion using a compositionally function $g$ and use node vectors as features for a classifier at that node. 
\begin{figure}[H]
\begin{center}
\includegraphics[width = 0.45\textwidth]{pic/trigram.png}
\caption{\label{trigram}Trigram Example (Socher et al., 2013) }
\end{center}
\end{figure}

RNTNs use the following equations to compute the parent vectors: 
\begin{equation*}
p_1 = f \left(  
\begin{bmatrix}
b \\ c
\end{bmatrix}^T
V^{[1:d]} 
\begin{bmatrix}
b \\ c
\end{bmatrix}
+ W
\begin{bmatrix}
b \\ c
\end{bmatrix}
 \right)
\end{equation*}
 
where $f = \textrm{tanh}$ is standard element-wise nonlinearity. $V^{[1:d] \in \mathbb{R}^{2d \times 2d \times d}}$ is the tensor that defines mulitiple bilinear forms. $W \in \mathbb{R}^{d \times 2d}$ is the main parameter to learn. 

The next parent vector $p_2$ in the tri-gram will be computed with the same weights:
\begin{equation*}
p_2 = f \left(  
\begin{bmatrix}
a \\ p_1
\end{bmatrix}^T
V^{[1:d]} 
\begin{bmatrix}
a \\ p_1
\end{bmatrix}
+ W
\begin{bmatrix}
a \\ p_1
\end{bmatrix}
 \right)
\end{equation*}

As we use the RNTN model as a black box in this project, so I skip the details on how to train the model. Interested reader could refer to the paper by Socher et al,. (2013). 



%<<<<<<< HEAD
%=======
%\begin{description}
%\item[General features] First we replace all the remaining HTML entities like ``\&amp;'' to ASCII code ``-''. We also delete the quotes around the whole tweet. Then we transform all the URL address to the keyword ``URL'' and the user names(start with @) to the key word ``target'' to keep the structure of the sentences. Hashtags may contain sentimental information so we simply remove the ``\#'' in the front of the hashtag.
%\item[Preprocessing with dictionary] Tweets contain very casual language. One common phenomenon is lengthening word. For example, instead of ``so'', Twitter users may repeat 'o's to express their intense feeling as ``sooo'' or ``sooooooo''. We transform all the lengthening words to three repeated letters like ``sooo''. There are also many misspellings and slangs in Twitter and we use a slang dictionary to formalize it.
%\item[Preprocessing for SVM] We extract additional features for SVM and attach ``\_neg'' to negated sentences. Detailed information can be found in Section \ref{svm}.  %2.2.1.
%\item[Preprocessing for RNTN] Since RNTN is based on Stanford parser, it may not be good at unknown words or incomplete sentences. Emoticons, for example, can not be organized but provide sentimental information. We split each emoticon with hat, eyes, nose and mouth and replace it with the corresponding adverb. For instance, ``:-)'' will be replaced with ``happily'', ``:O'' with be substituted as ``surprisedly''. Multiple punctuation like ``!!!!'' will be shortened to ``!''. To avoid errors in sentence splitter,  a period is added to each tweet that does not end with punctuation.
%\item[Word Cluster] We also try to reduce feature space with word cluster \cite{Owoputi:2013}. Firstly, we obtain hierarchical word clusters via Brown clustering. The algorithm partitions words into a base set of 1,000 clusters, and induces a hierarchy among those 1,000 clusters with a series of greedy agglomerative merges that heuristically optimize the likelihood of a hidden Markov model with a one-class-per-lexical-type constraint. Then we sort the word in each cluster with frequency and use the most frequent one to represent the sentiment of the cluster. According to our experiment, word cluster does help with nonstandard expressions like 
%
%\begin{verbatim}
%^001010110 never neva nvr nevr #never neverr nver neverrr nevaa neeever nevet neeeever nevva
%\end{verbatim}
%
%But it also integrates antonyms, which is fatal for our task.
%\begin{verbatim}
%^111100010011 sad blessed frightening sad lucky frustrating helpful impressed low steady :((( d&d busy
%\end{verbatim}
%
%
%
%\end{description}
%%>>>>>>> FETCH_HEAD


%Describe in reasonable detail the algorithm you are using to address this problem. A psuedocode description of the algorithm you are using is frequently useful. Trace through a concrete example, showing how your algorithm processes this example. The example should be complex enough to illustrate all of the important aspects of the problem but simple enough to be easily understood. If possible, an intuitively meaningful example is better than one with meaningless symbols. 

