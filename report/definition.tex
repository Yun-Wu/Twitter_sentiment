\section{Problem Definition and Algorithm}
\label{sec:def}

\subsection{Task Definition}
In this paper, we address the problem of sentiment analysis of Twitter message. To be more precise, 
given a message, we want to classify whether the message is of positive or negative (binary decision), or neutral sentiment (ternary decision). For the following two example messages, we are expected to return positive on the first one and negative on the second. 
\begin{mdframed}[
  leftmargin=\parindent,
  rightmargin=\parindent,
  skipabove=\topsep,
  skipbelow=\topsep
  ]
  If u haven't seen \#Rio2 yet-GO! You need to meet Gabi! Great singer. Cute. Absolutely hysterical! @KChenoweth \url{pic.twitter.com/kkVBUKjqE3}
\end{mdframed}
\vspace{-2mm}
\begin{mdframed}[
  leftmargin=\parindent,
  rightmargin=\parindent,
  skipabove=\topsep,
  skipbelow=\topsep
  ]
 The rio 2 has one of the worst soundtracks evvvvaaa. I'm at Alamo @Drafthouse Cinema. @marissanicole11 \url{http://4sq.com/1kKF8qE} 
\end{mdframed}

This is an interesting task because sentiment of Twitter message can be used as a barometer for public mood and opinion in diverse areas such as entertainment, politics and economics. For example, 
it was used to provide information on the temporal dynamic of sentiment in reaction to the debate video between Barack Obama and John McCain\cite{Diakopoulos:2010}. 
There is also a report indicates that sentiment toward public figure may have potential influence over stock market.\footnote{\url{http://goo.gl/WlfY1c}/} 

However, twitter message presents greater challenges for sentiment analysis than traditional text genres, such as newswire data.  Tweets are within 140 characters, often consists of a few short sentences or even a single sentence. 
The language used is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, such as, RT for “re-tweet” and \#hashtags, which are a type of tagging for Twitter messages. \footnote{\url{http://www.cs.york.ac.uk/semeval-2013/task2/}}


\subsection{Algorithm Definition}
We experiment with two different types of models: The first one is feature based model. 
In this model, we use different combination of features and SVM 
classifier. 
 The other is the recently proposed Recursive Neural Tensor Network which works pretty good on movie reviews. We first introduce the pre-processing steps and then the models. 

\subsubsection{Pre-processing}
The Twitter language is known as informal and flexible. Such properties will provide information about sentiment and should be processed carefully. As the feature and RNTN models are built upon two different basic ideas, we need to have different pre-processing steps for each model. For the feature based model, our goal is to maximize the information about sentiment while reduce the sparsity of the feature vector. For RNTN, we try to formalize tweets to well-organized sentences such that Stanford parser can recognize it well.

\begin{description}
\item[General features] First we replace all the remaining HTML entities like ``\&amp;'' to ASCII code ``-''. We also delete the quotes around the whole tweet. Then we transform all the URL addresses to the keyword ``URL'' and the user names(start with @) to the key word ``target'' to keep the structure of the sentence. Hashtags may contain sentimental information so we simply remove the ``\#'' in the front of the hashtag. We transform all the lengthening words ("sooooo" instead of "so") to three repeated letters like ``sooo''. There are also many misspellings and slangs in Twitter and we use a slang dictionary to formalize it.

\item[Word Cluster] We also try to reduce feature space with word cluster \cite{Owoputi:2013}. Firstly, we obtain hierarchical word clusters via Brown clustering. The algorithm partitions words into a base set of 1,000 clusters, and induces a hierarchy among those 1,000 clusters with a series of greedy agglomerative merges that heuristically optimize the likelihood of a hidden Markov model with a one-class-per-lexical-type constraint. Then we sort the words in each cluster with frequency and use the most frequent one to represent the sentiment of the cluster. According to our experiment, word cluster does help with nonstandard expressions: 
\vspace{-5mm}
\begin{mdframed}[
  ]
\{001010110\} never neva nvr nevr \#never neverr nver neverrr nevaa neeever nevet
\end{mdframed}
It also integrates antonyms:
\begin{mdframed}[
  ]
\{1111000100011\} sad blessed frightening lucky frustrating helpful impressed :((( 
\end{mdframed}




%\item[Preprocessing with dictionary] Tweets contain very casual language. One common phenomenon is lengthening word. For example, instead of ``so'', Twitter users may repeat 'o's to express their intense feeling as ``sooo'' or ``sooooooo''. We transform all the lengthening words to three repeated letters like ``sooo''. There are also many misspellings and slangs in Twitter and we use a slang dictionary to formalize it.
\item[Preprocessing for SVM] We extract additional features for SVM and attach ``\_neg'' to negated sentences. Detailed information can be found in Section \ref{svm}.  %2.2.1.
%We also try to reduce feature space with word cluster \cite{Owoputi:2013}. Firstly, we obtain hierarchical word clusters via Brown clustering. The algorithm partitions words into a base set of 1,000 clusters, and induces a hierarchy among those 1,000 clusters with a series of greedy agglomerative merges that heuristically optimize the likelihood of a hidden Markov model with a one-class-per-lexical-type constraint. Then we sort the word in each cluster with frequency and use the most frequent one to represent the sentiment of the cluster.

%\item[Word Cluster] We also try to reduce feature space with word cluster \cite{Owoputi:2013}. Firstly, we obtain hierarchical word clusters via Brown clustering. The algorithm partitions words into a base set of 1,000 clusters, and induces a hierarchy among those 1,000 clusters with a series of greedy agglomerative merges that heuristically optimize the likelihood of a hidden Markov model with a one-class-per-lexical-type constraint. Then we sort the word in each cluster with frequency and use the most frequent one to represent the sentiment of the cluster.\\
%\begin{verbatim}
%001010110 never neva nvr nevr \#never neverr nver neverrr nevaa neeever nevet neeeever nevva
%\end{verbatim}
%
%\\
%But it also integrates antonyms, which is fatal for our task.
%\begin{verbatim} 
%\\
%111100010011 sad blessed frightening sad lucky frustrating helpful impressed low steady :((( d\&d busy
%\end{verbatim}

\item[Preprocessing for RNTN] Since RNTN is based on Stanford parser, we need to preserve the structure of sentences while filtering out noise. 
 Emoticons, for example, can not be easily organized by our parser but provide sentimental information. We replace each emoticon with the corresponding adverb. For instance, ``:-)'' will be replaced with ``happily'', ``:O'' with be substituted as ``surprisedly''.  Also, to reduce errors in the sentence splitter, a period is added to each tweet that does not end with a punctuation.


\end{description}


\subsubsection{SVM}
\label{svm}
A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. We use the SVM$^{\text{libSVM}}$ software with a linear and Radial Basis Function (RBF) kernel. We input two sets of vectors of size $m$. Each entry in the vector corresponds to the presence of a feature. For example, with a unigram feature extractor, each feature is a single word found in a tweet. If this single word also appears in our dictionary, the value is $1$, otherwise the value is $0$. In our experiment, we evaluate the following features:
\begin{enumerate}
\item \textbf{Word unigram and bigram} (Unigram or Bigram): We use three different methods to present unigram feature. \\
 Bianry Format: We use $0/1$ to indicate the presence of a word.\\ 
 Frequency: The count of each word is used as unigram feature. For example, if “good” appears twice in the Tweet, we set 2 to this word.\\
 tf-idf: It is a numerical statistic to reflect the importance of a word to a document. tf-idf is the product of two statistics, term frequency and inverse document frequency. 
  \begin{eqnarray}
      tfidf_{i,j} = & tf_{i,j} \times idf_{i} &  \\
      tf_{i,j} = & \frac{n_{i,j}}{\sum_k n_{k,j}} & \\
      idf_i = & \log \frac{|D|}{|\{j: t_i \in d_j\}|} &
  \end{eqnarray}   
  	\item \textbf{Elongated word} (-elongated): Lengthening words by repeating letters (such as “coooool”, “goooood”) is reported to be a strong indication of sentiment. Brody et al. (2011) reported that on average, one out of six sentences has word lengthening. In our experiment, we also take elongated word as a feature.    \item \textbf{Targets, Hashtags, URLs, Numbers} (-url-num): These are twitter specific features. Users of Twitter use the "@" symbol to refer to other users and use hashtags “\#” to mark topics. 
  	%This is primarily done to increase the visibility of their tweets. URL and numbers are common in tweet.    \item \textbf{Negation} (neg): The effect of negation word is not simply the opposite of the original sentiment. In this paper, we try to append an NEG suffix to every word appearing between a negation and a clause-level punctuation mark. 
  
\end{enumerate}


\subsubsection{RNTN}
In Recursive Neural Tensor Network, each word is represented as a $d-$dimensional vector. When an $n-$gram is given to the compositional models, it is parsed into a binary tree (as in Figure \ref{trigram}). We compute the parent vector in a bottom up fashion using a compositionally function $g$ and use node vectors as features for a classifier at that node. 
\begin{figure}[H]
\begin{center}
\includegraphics[width = 0.33\textwidth]{pic/trigram.png}
\caption{\label{trigram}Trigram Example (Socher et al., 2013) }
\end{center}
\end{figure}

RNTNs use the following equations to compute the parent vectors: 
\begin{equation*}
p_1 = f \left(  
\begin{bmatrix}
b \\ c
\end{bmatrix}^T
V^{[1:d]} 
\begin{bmatrix}
b \\ c
\end{bmatrix}
+ W
\begin{bmatrix}
b \\ c
\end{bmatrix}
 \right)
\end{equation*}
 
where $f = \textrm{tanh}$ is standard element-wise nonlinearity. $V^{[1:d] \in \mathbb{R}^{2d \times 2d \times d}}$ is the tensor that defines mulitiple bilinear forms. $W \in \mathbb{R}^{d \times 2d}$ is the main parameter to learn. 

The next parent vector $p_2$ in the tri-gram will be computed with the same weights:
\begin{equation*}
p_2 = f \left(  
\begin{bmatrix}
a \\ p_1
\end{bmatrix}^T
V^{[1:d]} 
\begin{bmatrix}
a \\ p_1
\end{bmatrix}
+ W
\begin{bmatrix}
a \\ p_1
\end{bmatrix}
 \right)
\end{equation*}

As we use the RNTN model as a black box in this project, so I skip the details on how to train the model. Interested reader could refer to the paper by Socher et al,. (2013). 



%<<<<<<< HEAD
%=======
%\begin{description}
%\item[General features] First we replace all the remaining HTML entities like ``\&amp;'' to ASCII code ``-''. We also delete the quotes around the whole tweet. Then we transform all the URL address to the keyword ``URL'' and the user names(start with @) to the key word ``target'' to keep the structure of the sentences. Hashtags may contain sentimental information so we simply remove the ``\#'' in the front of the hashtag.
%\item[Preprocessing with dictionary] Tweets contain very casual language. One common phenomenon is lengthening word. For example, instead of ``so'', Twitter users may repeat 'o's to express their intense feeling as ``sooo'' or ``sooooooo''. We transform all the lengthening words to three repeated letters like ``sooo''. There are also many misspellings and slangs in Twitter and we use a slang dictionary to formalize it.
%\item[Preprocessing for SVM] We extract additional features for SVM and attach ``\_neg'' to negated sentences. Detailed information can be found in Section \ref{svm}.  %2.2.1.
%\item[Preprocessing for RNTN] Since RNTN is based on Stanford parser, it may not be good at unknown words or incomplete sentences. Emoticons, for example, can not be organized but provide sentimental information. We split each emoticon with hat, eyes, nose and mouth and replace it with the corresponding adverb. For instance, ``:-)'' will be replaced with ``happily'', ``:O'' with be substituted as ``surprisedly''. Multiple punctuation like ``!!!!'' will be shortened to ``!''. To avoid errors in sentence splitter,  a period is added to each tweet that does not end with punctuation.
%\item[Word Cluster] We also try to reduce feature space with word cluster \cite{Owoputi:2013}. Firstly, we obtain hierarchical word clusters via Brown clustering. The algorithm partitions words into a base set of 1,000 clusters, and induces a hierarchy among those 1,000 clusters with a series of greedy agglomerative merges that heuristically optimize the likelihood of a hidden Markov model with a one-class-per-lexical-type constraint. Then we sort the word in each cluster with frequency and use the most frequent one to represent the sentiment of the cluster. According to our experiment, word cluster does help with nonstandard expressions like 
%
%\begin{verbatim}
%^001010110 never neva nvr nevr #never neverr nver neverrr nevaa neeever nevet neeeever nevva
%\end{verbatim}
%
%But it also integrates antonyms, which is fatal for our task.
%\begin{verbatim}
%^111100010011 sad blessed frightening sad lucky frustrating helpful impressed low steady :((( d&d busy
%\end{verbatim}
%
%
%
%\end{description}
%%>>>>>>> FETCH_HEAD


%Describe in reasonable detail the algorithm you are using to address this problem. A psuedocode description of the algorithm you are using is frequently useful. Trace through a concrete example, showing how your algorithm processes this example. The example should be complex enough to illustrate all of the important aspects of the problem but simple enough to be easily understood. If possible, an intuitively meaningful example is better than one with meaningless symbols. 

